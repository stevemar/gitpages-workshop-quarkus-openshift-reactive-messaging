{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Workshop: Reactive Messaging with Quarkus on OpenShift \u00b6 In this workshop you'll learn how to implement reactive messaging functionality with Java, Quarkus , Kafka , Vert.x and MicroProfile . An end-to-end sample application will be deployed to Red Hat OpenShift . The code is available as open source as part of the Cloud Native Starter project. One benefit of reactive models is the ability to update web applications by sending messages, rather than pulling for updates. This is more efficient and improves the user experience. The workshop uses a sample application to demonstrate reactive functionality. The simple application displays links to articles and author information. Articles can be created via REST API. The web application receives a notification and adds the new article to the page. The animation shows how curl requests are executed at the bottom which trigger updates to the web application at the top. Architecture \u00b6 The next diagram explains the flow between the different components and microservices. API client 'Submissions' triggers the REST API of the 'Articles' service to create new articles. The 'Articles' service sends a message to the 'Web-API' service via 'Kafka'. The 'Web-API' provides a streaming endpoint The web application 'Web-App' consumes the streaming endpoint Note that in this workshop you will deploy the full application as described in the previous diagram. But to simplify the workshop you'll re-implement a simpler version of the 'Web-API' service which only invokes the 'Articles' service. Estimated time and level \u00b6 Time Level one hour beginners Objectives \u00b6 After you complete this workshop, you'll understand the following reactive functionality: Sending and receiving Kafka messages via MicroProfile Sending events from microservices to web applications via Server Sent Events Sending in-memory messages via MicroProfile and Vert.x Event Bus The intention of this workshop is not to explain every aspect of reactive programming, but to explain core reactive principles and to deploy a complete reactive application which you can inspect after the workshop in more detail. About this workshop \u00b6 The introductory page of the workshop is broken down into the following sections: Agenda Compatibility Technology Used Credits What`s next? Agenda \u00b6 These are the labs of this workshop, go through all of them in sequence, start with pre work: Pre work Create your Cloud Environment Exercise 1 Deploy example application via Script Exercise 2 Reactive Messaging with MicroProfile Exercise 3 Server Sent Events Exercise 4 Vert.x Event Bus Exercise 5 (optional) Use distributed Logging Compatibility \u00b6 This workshop has been tested on the following platforms: IBM Open Shift : version 4.3 IBM Cloud Shell : beta Technology Used \u00b6 Jakarta EE MicroProfile Quarkus Apache Kafka PostgresSQL RedHat OpenShift Microservices architecture Vue.js Credits \u00b6 Niklas Heidloff Harald Uebele Thomas S\u00fcdbr\u00f6cker What`s next? \u00b6 The blogs as well as the presentation describe the functionality in more detail.","title":"About the workshop"},{"location":"#workshop-reactive-messaging-with-quarkus-on-openshift","text":"In this workshop you'll learn how to implement reactive messaging functionality with Java, Quarkus , Kafka , Vert.x and MicroProfile . An end-to-end sample application will be deployed to Red Hat OpenShift . The code is available as open source as part of the Cloud Native Starter project. One benefit of reactive models is the ability to update web applications by sending messages, rather than pulling for updates. This is more efficient and improves the user experience. The workshop uses a sample application to demonstrate reactive functionality. The simple application displays links to articles and author information. Articles can be created via REST API. The web application receives a notification and adds the new article to the page. The animation shows how curl requests are executed at the bottom which trigger updates to the web application at the top.","title":"Workshop: Reactive Messaging with Quarkus on OpenShift"},{"location":"#architecture","text":"The next diagram explains the flow between the different components and microservices. API client 'Submissions' triggers the REST API of the 'Articles' service to create new articles. The 'Articles' service sends a message to the 'Web-API' service via 'Kafka'. The 'Web-API' provides a streaming endpoint The web application 'Web-App' consumes the streaming endpoint Note that in this workshop you will deploy the full application as described in the previous diagram. But to simplify the workshop you'll re-implement a simpler version of the 'Web-API' service which only invokes the 'Articles' service.","title":"Architecture"},{"location":"#estimated-time-and-level","text":"Time Level one hour beginners","title":"Estimated time and level"},{"location":"#objectives","text":"After you complete this workshop, you'll understand the following reactive functionality: Sending and receiving Kafka messages via MicroProfile Sending events from microservices to web applications via Server Sent Events Sending in-memory messages via MicroProfile and Vert.x Event Bus The intention of this workshop is not to explain every aspect of reactive programming, but to explain core reactive principles and to deploy a complete reactive application which you can inspect after the workshop in more detail.","title":"Objectives"},{"location":"#about-this-workshop","text":"The introductory page of the workshop is broken down into the following sections: Agenda Compatibility Technology Used Credits What`s next?","title":"About this workshop"},{"location":"#agenda","text":"These are the labs of this workshop, go through all of them in sequence, start with pre work: Pre work Create your Cloud Environment Exercise 1 Deploy example application via Script Exercise 2 Reactive Messaging with MicroProfile Exercise 3 Server Sent Events Exercise 4 Vert.x Event Bus Exercise 5 (optional) Use distributed Logging","title":"Agenda"},{"location":"#compatibility","text":"This workshop has been tested on the following platforms: IBM Open Shift : version 4.3 IBM Cloud Shell : beta","title":"Compatibility"},{"location":"#technology-used","text":"Jakarta EE MicroProfile Quarkus Apache Kafka PostgresSQL RedHat OpenShift Microservices architecture Vue.js","title":"Technology Used"},{"location":"#credits","text":"Niklas Heidloff Harald Uebele Thomas S\u00fcdbr\u00f6cker","title":"Credits"},{"location":"#whats-next","text":"The blogs as well as the presentation describe the functionality in more detail.","title":"What`s next?"},{"location":"SUMMARY/","text":"Summary \u00b6 Setup the IBM Cloud Environment \u00b6 Introduction Access the Cluster Access IBM Cloud Shell Setup the sample application \u00b6 Exercise 1 - Setup via Script YouTube - How to setup the sample application (optional) Build new version of the Microservice \u00b6 Exercise 2 - Reactive Messaging with MicroProfile Exercise 3 - Server Sent Events Exercise 4 - Vert.x Event Bus Exercise 5 (optional) - Use distributed Logging Resources \u00b6 YouTube video How to setup the reactive sample application on OpenShift Blog posts related to reactive Workshop: Reactive Endpoint with Quarkus on OpenShift Cloud-Native-Starter project Cloud-Native-Starter project reactive","title":"Summary"},{"location":"SUMMARY/#summary","text":"","title":"Summary"},{"location":"SUMMARY/#setup-the-ibm-cloud-environment","text":"Introduction Access the Cluster Access IBM Cloud Shell","title":"Setup the IBM Cloud Environment"},{"location":"SUMMARY/#setup-the-sample-application","text":"Exercise 1 - Setup via Script YouTube - How to setup the sample application (optional)","title":"Setup the sample application"},{"location":"SUMMARY/#build-new-version-of-the-microservice","text":"Exercise 2 - Reactive Messaging with MicroProfile Exercise 3 - Server Sent Events Exercise 4 - Vert.x Event Bus Exercise 5 (optional) - Use distributed Logging","title":"Build new version of the Microservice"},{"location":"SUMMARY/#resources","text":"YouTube video How to setup the reactive sample application on OpenShift Blog posts related to reactive Workshop: Reactive Endpoint with Quarkus on OpenShift Cloud-Native-Starter project Cloud-Native-Starter project reactive","title":"Resources"},{"location":"exercise-01/","text":"Exercise 1: Deploy the example application via script \u00b6 In this short exercise you'll deploy Kafka, PostgreSQL, and the Microservices of the reactive example via a script and test the example application. Step 1: Deploy the example application \u00b6 Invoke the following command: bash ~/cloud-native-starter/reactive/os4-scripts/deploy-example.sh The deployment takes approximately 10 minutes . The script console output shows links to the Microservices and the Web-App at the end of the execution. The following 35 sec YouTube video shows a example execution of the script and the test of the application. Step 2: Launch the web application \u00b6 To launch the web application get the URL from the last output and open the application in a browser. Step 3: Copy the curl command \u00b6 Copy the curl command to create a new article and insert it into your current terminal session. Step 4: Test the example application \u00b6 Open the web application in a browser. Then invoke the curl post command. The web application should show the new entry. Step 5 (optional): Verify the deployed PostgreSQL \u00b6 You can check the status via the OpenShift web console. On the 'Pods' page select the 'postgres' project. Step 6 (optional): Verify the deployed Kafka \u00b6 You can check the status via the OpenShift web console. On the 'Pods' page select the 'kafka' project. Step 7 (optional): Verify the deployed Services and Web Application \u00b6 Make sure all four pods in the 'cloud-native-starter' project are running. The previous steps have create build configs, builds and image streams.","title":"Lab 1. Setup via Script"},{"location":"exercise-01/#exercise-1-deploy-the-example-application-via-script","text":"In this short exercise you'll deploy Kafka, PostgreSQL, and the Microservices of the reactive example via a script and test the example application.","title":"Exercise 1: Deploy the example application via script"},{"location":"exercise-01/#step-1-deploy-the-example-application","text":"Invoke the following command: bash ~/cloud-native-starter/reactive/os4-scripts/deploy-example.sh The deployment takes approximately 10 minutes . The script console output shows links to the Microservices and the Web-App at the end of the execution. The following 35 sec YouTube video shows a example execution of the script and the test of the application.","title":"Step 1: Deploy the example application"},{"location":"exercise-01/#step-2-launch-the-web-application","text":"To launch the web application get the URL from the last output and open the application in a browser.","title":"Step 2: Launch the web application"},{"location":"exercise-01/#step-3-copy-the-curl-command","text":"Copy the curl command to create a new article and insert it into your current terminal session.","title":"Step 3: Copy the curl command"},{"location":"exercise-01/#step-4-test-the-example-application","text":"Open the web application in a browser. Then invoke the curl post command. The web application should show the new entry.","title":"Step 4: Test the example application"},{"location":"exercise-01/#step-5-optional-verify-the-deployed-postgresql","text":"You can check the status via the OpenShift web console. On the 'Pods' page select the 'postgres' project.","title":"Step 5 (optional): Verify the deployed PostgreSQL"},{"location":"exercise-01/#step-6-optional-verify-the-deployed-kafka","text":"You can check the status via the OpenShift web console. On the 'Pods' page select the 'kafka' project.","title":"Step 6 (optional): Verify the deployed Kafka"},{"location":"exercise-01/#step-7-optional-verify-the-deployed-services-and-web-application","text":"Make sure all four pods in the 'cloud-native-starter' project are running. The previous steps have create build configs, builds and image streams.","title":"Step 7 (optional): Verify the deployed Services and Web Application"},{"location":"exercise-02/","text":"Exercise 2: Reactive Messaging with MicroProfile \u00b6 In this lab you'll learn how to use reactive messaging with MicroProfile Messaging . With simple Java annotations messages can be sent and received in memory as well as via Apache Kafka . MicroProfile Messaging implements the Reactive Streams standard which defines how to do asynchronous stream processing for different programming languages independently from specific libraries. The interfaces of the main Reactive Streams components Subscriber, Publisher and Processor have been available since JDK 9. The implementation of these interfaces is provided by MicroProfile. The 'Articles' service writes messages to Kafka after new articles have been created. In this lab we'll take a look how these messages can be read from the 'Web-API' service. Step 1: Modify Subscriber, Publisher and Processor Class \u00b6 Let's take a look at a Java class which receives incomping messages from Kafka and sends outgoing in-memory messages. Invoke the following command in the Cloud Shell. cd ~/cloud-native-starter/reactive/web-api-reactive/src/main/java/com/ibm/webapi/apis nano NewArticleListener.java The @Incoming annotation indicates that the method consumes the items from the topic 'new-article-created'. The @Outgoing annotation indicates that the objects returned by the method are sent to the stream 'stream-new-article'. @Broadcast indicates that the item are dispatched to all subscribers. The snippet is a Subscriber as well as a Publisher which means that it is automatically also a Processor which can, for example, convert incoming messages and forward them. Let's make a simple change and redeploy the microservice by adding this line. System.out.println ( \"Here you can add process functionality\" ) ; Exit the Editor via 'Ctrl-X', 'y' and 'Enter'. Confirm that the changes have been saved. cd ~/cloud-native-starter/reactive/web-api-reactive/src/main/java/com/ibm/webapi/apis cat NewArticleListener.java Step 2: Configure Kafka \u00b6 The incoming messages in the snippet above are received from Kafka. The 'Articles' service writes these messages to Kafka after new articles have been created. In order to subscribe to these Kafka messages in Quarkus, the topic needs to be configured in application.properties. cd ~/cloud-native-starter/reactive/web-api-reactive/src/main/resources cat application.properties cd ~/cloud-native-starter/reactive/web-api-reactive/src/main/java/com/ibm/webapi/apis cat NewArticleListener.java Step 3: Deploy new Version \u00b6 cd ~/cloud-native-starter/reactive/web-api-reactive oc start-build web-api-reactive --from-dir = . On the 'Builds' page wait until the new build has been completed. Once completed, delete the 'Web-API' pod which causes a new pod with the latest image to be started. Step 4: Verify the new Version \u00b6 Create a new article by invoking a curl post command. You can get the URL from the script show-urls. ~/cloud-native-starter/reactive/os4-scripts/show-urls.sh In order to see the logs, you can do two things: Use the following instructions which leverage a terminal Use distributed logging as documented in Lab 5 In the terminal get the pod name: oc get pods After this invoke this command to display the logs of the pod. oc logs web-api-reactive-xxxxxxxxxxx-xxxxx Your added line shows up in the logs now.","title":"Lab 2. Reactive Messaging with MicroProfile"},{"location":"exercise-02/#exercise-2-reactive-messaging-with-microprofile","text":"In this lab you'll learn how to use reactive messaging with MicroProfile Messaging . With simple Java annotations messages can be sent and received in memory as well as via Apache Kafka . MicroProfile Messaging implements the Reactive Streams standard which defines how to do asynchronous stream processing for different programming languages independently from specific libraries. The interfaces of the main Reactive Streams components Subscriber, Publisher and Processor have been available since JDK 9. The implementation of these interfaces is provided by MicroProfile. The 'Articles' service writes messages to Kafka after new articles have been created. In this lab we'll take a look how these messages can be read from the 'Web-API' service.","title":"Exercise 2: Reactive Messaging with MicroProfile"},{"location":"exercise-02/#step-1-modify-subscriber-publisher-and-processor-class","text":"Let's take a look at a Java class which receives incomping messages from Kafka and sends outgoing in-memory messages. Invoke the following command in the Cloud Shell. cd ~/cloud-native-starter/reactive/web-api-reactive/src/main/java/com/ibm/webapi/apis nano NewArticleListener.java The @Incoming annotation indicates that the method consumes the items from the topic 'new-article-created'. The @Outgoing annotation indicates that the objects returned by the method are sent to the stream 'stream-new-article'. @Broadcast indicates that the item are dispatched to all subscribers. The snippet is a Subscriber as well as a Publisher which means that it is automatically also a Processor which can, for example, convert incoming messages and forward them. Let's make a simple change and redeploy the microservice by adding this line. System.out.println ( \"Here you can add process functionality\" ) ; Exit the Editor via 'Ctrl-X', 'y' and 'Enter'. Confirm that the changes have been saved. cd ~/cloud-native-starter/reactive/web-api-reactive/src/main/java/com/ibm/webapi/apis cat NewArticleListener.java","title":"Step 1: Modify Subscriber, Publisher and Processor Class"},{"location":"exercise-02/#step-2-configure-kafka","text":"The incoming messages in the snippet above are received from Kafka. The 'Articles' service writes these messages to Kafka after new articles have been created. In order to subscribe to these Kafka messages in Quarkus, the topic needs to be configured in application.properties. cd ~/cloud-native-starter/reactive/web-api-reactive/src/main/resources cat application.properties cd ~/cloud-native-starter/reactive/web-api-reactive/src/main/java/com/ibm/webapi/apis cat NewArticleListener.java","title":"Step 2: Configure Kafka"},{"location":"exercise-02/#step-3-deploy-new-version","text":"cd ~/cloud-native-starter/reactive/web-api-reactive oc start-build web-api-reactive --from-dir = . On the 'Builds' page wait until the new build has been completed. Once completed, delete the 'Web-API' pod which causes a new pod with the latest image to be started.","title":"Step 3: Deploy new Version"},{"location":"exercise-02/#step-4-verify-the-new-version","text":"Create a new article by invoking a curl post command. You can get the URL from the script show-urls. ~/cloud-native-starter/reactive/os4-scripts/show-urls.sh In order to see the logs, you can do two things: Use the following instructions which leverage a terminal Use distributed logging as documented in Lab 5 In the terminal get the pod name: oc get pods After this invoke this command to display the logs of the pod. oc logs web-api-reactive-xxxxxxxxxxx-xxxxx Your added line shows up in the logs now.","title":"Step 4: Verify the new Version"},{"location":"exercise-03/","text":"Exercise 3: Server Sent Events \u00b6 In this lab you'll learn how to expose streaming endpoints so that web applications are notified via Server Sent Events . The web application 'Web-App' receives notifications from the 'Web-API' service. Step 1: Understand the Web Application Consumer \u00b6 Let's take a look at the JavaScript code which consumes the server side events. A new EventSource is created by passing in the the URL of the streaming endpoint. The function source.onmessage is invoked when the events arrive. In our case this triggers the reload of the last articles. cd ~/cloud-native-starter/reactive/web-app-reactive/src/components cat Home.vue Step 2: Develop the Streaming Endpoint \u00b6 The sample already comes with a working endpoint. Let's delete the file and recreate it from scratch. cd ~/cloud-native-starter/reactive/web-api-reactive/src/main/java/com/ibm/webapi/apis/ rm NewArticlesStreamResource.java touch NewArticlesStreamResource.java nano NewArticlesStreamResource.java Add the package name, the import statements and the empty class. package com.ibm.webapi.apis ; import javax.inject.Inject ; import javax.ws.rs.core.MediaType ; import org.reactivestreams.Publisher ; import io.smallrye.reactive.messaging.annotations.Channel ; import javax.ws.rs.GET ; import javax.ws.rs.Path ; import javax.ws.rs.Produces ; import org.jboss.resteasy.annotations.SseElementType ; @Path ( \"/v2\" ) public class NewArticlesStreamResource { } In Lab 2 you saw how to publish messages to the in-memory channel 'stream-new-article'. A publisher to this channel can easily be injected via @Inject and @Channel. @Inject @Channel ( \"stream-new-article\" ) Publisher < String > newArticles ; Last, but not least, add the implementation of the streaming endpoint. The media type is MediaType.SERVER_SENT_EVENTS and the annotation @SseElementType defines the type. @GET @Path ( \"/server-sent-events\" ) @Produces ( MediaType . SERVER_SENT_EVENTS ) @SseElementType ( \"text/plain\" ) public Publisher < String > stream () { return newArticles ; } Once you've entered everything the class should look like this. Exit the Editor via 'Ctrl-X', 'y' and 'Enter'. Step 3: Deploy new Version \u00b6 cd ~/cloud-native-starter/reactive/web-api-reactive oc start-build web-api-reactive --from-dir = . On the 'Builds' page wait until the new build has been completed. Once completed, delete the 'Web-API' pod which causes a new pod with the latest image to be started. Step 4: Verify new Version \u00b6 Make sure all four pods in the 'cloud-native-starter' project are running. Note that it takes a couple of minutes until this happens. To launch the application get the URLs via the following command. ~/cloud-native-starter/reactive/os4-scripts/show-urls.sh Open the web application in a browser. Then invoke the curl post command. The web application should show the new entry.","title":"Lab 3. Server Sent Events"},{"location":"exercise-03/#exercise-3-server-sent-events","text":"In this lab you'll learn how to expose streaming endpoints so that web applications are notified via Server Sent Events . The web application 'Web-App' receives notifications from the 'Web-API' service.","title":"Exercise 3: Server Sent Events"},{"location":"exercise-03/#step-1-understand-the-web-application-consumer","text":"Let's take a look at the JavaScript code which consumes the server side events. A new EventSource is created by passing in the the URL of the streaming endpoint. The function source.onmessage is invoked when the events arrive. In our case this triggers the reload of the last articles. cd ~/cloud-native-starter/reactive/web-app-reactive/src/components cat Home.vue","title":"Step 1: Understand the Web Application Consumer"},{"location":"exercise-03/#step-2-develop-the-streaming-endpoint","text":"The sample already comes with a working endpoint. Let's delete the file and recreate it from scratch. cd ~/cloud-native-starter/reactive/web-api-reactive/src/main/java/com/ibm/webapi/apis/ rm NewArticlesStreamResource.java touch NewArticlesStreamResource.java nano NewArticlesStreamResource.java Add the package name, the import statements and the empty class. package com.ibm.webapi.apis ; import javax.inject.Inject ; import javax.ws.rs.core.MediaType ; import org.reactivestreams.Publisher ; import io.smallrye.reactive.messaging.annotations.Channel ; import javax.ws.rs.GET ; import javax.ws.rs.Path ; import javax.ws.rs.Produces ; import org.jboss.resteasy.annotations.SseElementType ; @Path ( \"/v2\" ) public class NewArticlesStreamResource { } In Lab 2 you saw how to publish messages to the in-memory channel 'stream-new-article'. A publisher to this channel can easily be injected via @Inject and @Channel. @Inject @Channel ( \"stream-new-article\" ) Publisher < String > newArticles ; Last, but not least, add the implementation of the streaming endpoint. The media type is MediaType.SERVER_SENT_EVENTS and the annotation @SseElementType defines the type. @GET @Path ( \"/server-sent-events\" ) @Produces ( MediaType . SERVER_SENT_EVENTS ) @SseElementType ( \"text/plain\" ) public Publisher < String > stream () { return newArticles ; } Once you've entered everything the class should look like this. Exit the Editor via 'Ctrl-X', 'y' and 'Enter'.","title":"Step 2: Develop the Streaming Endpoint"},{"location":"exercise-03/#step-3-deploy-new-version","text":"cd ~/cloud-native-starter/reactive/web-api-reactive oc start-build web-api-reactive --from-dir = . On the 'Builds' page wait until the new build has been completed. Once completed, delete the 'Web-API' pod which causes a new pod with the latest image to be started.","title":"Step 3: Deploy new Version"},{"location":"exercise-03/#step-4-verify-new-version","text":"Make sure all four pods in the 'cloud-native-starter' project are running. Note that it takes a couple of minutes until this happens. To launch the application get the URLs via the following command. ~/cloud-native-starter/reactive/os4-scripts/show-urls.sh Open the web application in a browser. Then invoke the curl post command. The web application should show the new entry.","title":"Step 4: Verify new Version"},{"location":"exercise-04/","text":"Exercise 4: Vert.x Event Bus \u00b6 The Vert.x Event Bus allows different parts of your application to communicate with each other. Check out the guide to find out more details. In this lab you'll learn how to use the bus to communicate in-memory between different layers of the 'Articles' service. The 'Articles' service uses a clean architecture approach. There are three different layers: API Business Data The API layer contains the implementation of the REST APIs and the external messaging interfaces. The data layer contains the implementation of the persistence and could include calls to other external services. The API layer and the data layer can be easily replaced without changing the business logic. In this lab you'll use the event bus to communicate between the business and the API layers. Step 1: Understand the Publisher \u00b6 Let's take a look at the implementation of ArticleService in the business layer. cd ~/cloud-native-starter/reactive/articles-reactive/src/main/java/com/ibm/articles/ cat business/ArticleService.java An instance of the bus can be injected via @Inject. Next let's modify the method 'sendMessageToKafka' slightly by adding a System.out.println. System.out.println(\"Sending message via Vert.x Event Bus\"); cd ~/cloud-native-starter/reactive/articles-reactive/src/main/java/com/ibm/articles/ nano business/ArticleService.java Exit the Editor via 'Ctrl-X', 'y' and 'Enter'. Step 2: Understand the Subscriber \u00b6 The method 'sendMessageToKafka' in the class NewArticleCreatedListener.java in the API layer is invoked when the messages should be sent to Kafka. This is defined via the annotation @ConsumeEvent. Let's modify this method slightly as well. System . out . println ( \"Receiving message via Vert.x Event Bus\" ); cd ~/cloud-native-starter/reactive/articles-reactive/src/main/java/com/ibm/articles/ nano apis/NewArticleCreatedListener.java Exit the Editor via 'Ctrl-X', 'y' and 'Enter'. Step 3: Deploy new Version \u00b6 cd ~/cloud-native-starter/reactive/articles-reactive oc start-build articles-reactive --from-dir = . Wait until the build has been completed. Delete the articles pod. This will trigger Kubernetes to start a new pod with the latest version of the image. Step 4: Deploy new Version \u00b6 Create a new article by invoking a curl post command. You can get the URL from the script show-urls. ~/cloud-native-starter/reactive/os4-scripts/show-urls.sh In order to see the logs, you can do two things: Use the following instructions which leverage a terminal Use distributed logging as documented in Lab 5 In the terminal get the pod name: oc get pods After this invoke this command to display the logs of the pod. oc logs articles-reactive-xxxxxxxxxxx-xxxxx Your added line shows up in the logs now.","title":"Lab 4. Vert.x Event Bus"},{"location":"exercise-04/#exercise-4-vertx-event-bus","text":"The Vert.x Event Bus allows different parts of your application to communicate with each other. Check out the guide to find out more details. In this lab you'll learn how to use the bus to communicate in-memory between different layers of the 'Articles' service. The 'Articles' service uses a clean architecture approach. There are three different layers: API Business Data The API layer contains the implementation of the REST APIs and the external messaging interfaces. The data layer contains the implementation of the persistence and could include calls to other external services. The API layer and the data layer can be easily replaced without changing the business logic. In this lab you'll use the event bus to communicate between the business and the API layers.","title":"Exercise 4: Vert.x Event Bus"},{"location":"exercise-04/#step-1-understand-the-publisher","text":"Let's take a look at the implementation of ArticleService in the business layer. cd ~/cloud-native-starter/reactive/articles-reactive/src/main/java/com/ibm/articles/ cat business/ArticleService.java An instance of the bus can be injected via @Inject. Next let's modify the method 'sendMessageToKafka' slightly by adding a System.out.println. System.out.println(\"Sending message via Vert.x Event Bus\"); cd ~/cloud-native-starter/reactive/articles-reactive/src/main/java/com/ibm/articles/ nano business/ArticleService.java Exit the Editor via 'Ctrl-X', 'y' and 'Enter'.","title":"Step 1: Understand the Publisher"},{"location":"exercise-04/#step-2-understand-the-subscriber","text":"The method 'sendMessageToKafka' in the class NewArticleCreatedListener.java in the API layer is invoked when the messages should be sent to Kafka. This is defined via the annotation @ConsumeEvent. Let's modify this method slightly as well. System . out . println ( \"Receiving message via Vert.x Event Bus\" ); cd ~/cloud-native-starter/reactive/articles-reactive/src/main/java/com/ibm/articles/ nano apis/NewArticleCreatedListener.java Exit the Editor via 'Ctrl-X', 'y' and 'Enter'.","title":"Step 2: Understand the Subscriber"},{"location":"exercise-04/#step-3-deploy-new-version","text":"cd ~/cloud-native-starter/reactive/articles-reactive oc start-build articles-reactive --from-dir = . Wait until the build has been completed. Delete the articles pod. This will trigger Kubernetes to start a new pod with the latest version of the image.","title":"Step 3: Deploy new Version"},{"location":"exercise-04/#step-4-deploy-new-version","text":"Create a new article by invoking a curl post command. You can get the URL from the script show-urls. ~/cloud-native-starter/reactive/os4-scripts/show-urls.sh In order to see the logs, you can do two things: Use the following instructions which leverage a terminal Use distributed logging as documented in Lab 5 In the terminal get the pod name: oc get pods After this invoke this command to display the logs of the pod. oc logs articles-reactive-xxxxxxxxxxx-xxxxx Your added line shows up in the logs now.","title":"Step 4: Deploy new Version"},{"location":"exercise-05/","text":"Exercise 5 (Optional): Use distributed Logging \u00b6 Cloud native applications based on microservices contain many parts that create logs. A logging service that is able to collect all distributed logs in one place is a highly recommended tool. There are many logging solutions that you can install directly into your Kubernetes or OpenShift cluster. But then you have an additional application that needs to be maintained and one that needs persistent storage as well to store logs for a period of time. IBM Cloud offers \"Logging as a Service\" in the form of IBM Log Analysis with LogDNA . It offers features to filter, search, and tail log data, define alerts, and design custom views to monitor application and system logs. You can test \"IBM Log Analysis with LogDNA\" for free with somewhat limited capabilities and we will show you in this lab how to connect your OpenShift cluster to an instance of it. Official documentation for setting up the LogDNA agent for an OpenShift cluster is here . For the following instructions use the IBM Cloud Shell to enter the commands. Step 1: Create LogDNA Service \u00b6 In your browser log in to the IBM Cloud dashboard . Make sure you are using your own account . From the 'burger menu' in the upper left corner select 'Observability'. Create an 'IBM Log Analysis with LogDNA' instance by clicking on 'Create new'. On the 'Create' tab leave all defaults. All you have to do is to create the big blue 'Create' button. Step 2: Configure LogDNA \u00b6 Select 'Edit log sources'. Select the 'OpenShift' tab. Copy, paste, and execute the commands into your IBM Cloud Shell: In the Cloud Shell check that the logging agent is running. oc get all -n ibm-observe Step 3: Use LogDNA \u00b6 Go back to the IBM Cloud dashboard . Make sure you are using your own account. From the 'burger menu' in the upper left corner select 'Observability' and then 'Logging'. Click 'View LogDNA'. In Lab 4 Deploying Sample Application you have deployed an instance of the 'Articles' service called 'articles-reactive'. We will check LogDNA for output from this instance. Execute the following commands in the Cloud Shell: oc project cloud-native-starter watch curl -X GET \"http://$(oc get route articles-reactive -o jsonpath={.spec.host})/v2/articles?amount=10\" -H \"accept: application/json\" The \"watch\" command will constantly (every 2 seconds) request articles information. Refresh your browser tab with the LogDNA dashboard and insert in the search field \"getArticlesReactive\". Note: If you don't see \"getArticlesReactive\" wait a little longer (with the free/lite version it can take several minutes before data shows up), then refresh the browser tab of the LogDNA dashboard again. Select 'Unsaved View' and then 'Save as new/alert'. Give the view a ane and press 'Save View'. From now the new view is available under 'Views'. Congratulations! You\u2019ve finished the workshop!","title":"(Optional) Lab 5. Use distributed Logging"},{"location":"exercise-05/#exercise-5-optional-use-distributed-logging","text":"Cloud native applications based on microservices contain many parts that create logs. A logging service that is able to collect all distributed logs in one place is a highly recommended tool. There are many logging solutions that you can install directly into your Kubernetes or OpenShift cluster. But then you have an additional application that needs to be maintained and one that needs persistent storage as well to store logs for a period of time. IBM Cloud offers \"Logging as a Service\" in the form of IBM Log Analysis with LogDNA . It offers features to filter, search, and tail log data, define alerts, and design custom views to monitor application and system logs. You can test \"IBM Log Analysis with LogDNA\" for free with somewhat limited capabilities and we will show you in this lab how to connect your OpenShift cluster to an instance of it. Official documentation for setting up the LogDNA agent for an OpenShift cluster is here . For the following instructions use the IBM Cloud Shell to enter the commands.","title":"Exercise 5 (Optional): Use distributed Logging"},{"location":"exercise-05/#step-1-create-logdna-service","text":"In your browser log in to the IBM Cloud dashboard . Make sure you are using your own account . From the 'burger menu' in the upper left corner select 'Observability'. Create an 'IBM Log Analysis with LogDNA' instance by clicking on 'Create new'. On the 'Create' tab leave all defaults. All you have to do is to create the big blue 'Create' button.","title":"Step 1: Create LogDNA Service"},{"location":"exercise-05/#step-2-configure-logdna","text":"Select 'Edit log sources'. Select the 'OpenShift' tab. Copy, paste, and execute the commands into your IBM Cloud Shell: In the Cloud Shell check that the logging agent is running. oc get all -n ibm-observe","title":"Step 2: Configure LogDNA"},{"location":"exercise-05/#step-3-use-logdna","text":"Go back to the IBM Cloud dashboard . Make sure you are using your own account. From the 'burger menu' in the upper left corner select 'Observability' and then 'Logging'. Click 'View LogDNA'. In Lab 4 Deploying Sample Application you have deployed an instance of the 'Articles' service called 'articles-reactive'. We will check LogDNA for output from this instance. Execute the following commands in the Cloud Shell: oc project cloud-native-starter watch curl -X GET \"http://$(oc get route articles-reactive -o jsonpath={.spec.host})/v2/articles?amount=10\" -H \"accept: application/json\" The \"watch\" command will constantly (every 2 seconds) request articles information. Refresh your browser tab with the LogDNA dashboard and insert in the search field \"getArticlesReactive\". Note: If you don't see \"getArticlesReactive\" wait a little longer (with the free/lite version it can take several minutes before data shows up), then refresh the browser tab of the LogDNA dashboard again. Select 'Unsaved View' and then 'Save as new/alert'. Give the view a ane and press 'Save View'. From now the new view is available under 'Views'. Congratulations! You\u2019ve finished the workshop!","title":"Step 3: Use LogDNA"},{"location":"pre-work/","text":"Lab 1: Create your Cloud Environment \u00b6 The main instructions of this workshop assume that you will use Red Hat OpenShift 4.3 on IBM Cloud. However you can also use CodeReady Containers to run OpenShift locally. To use OpenShift on IBM Cloud and LogDNA in lab 8, an IBM Cloud account is needed. It's free, doesn't expire and for the lite account no credit card is required. We will use preconfigured OpenShift on IBM Cloud clusters in this hands-on workshop.","title":"Introduction"},{"location":"pre-work/#lab-1-create-your-cloud-environment","text":"The main instructions of this workshop assume that you will use Red Hat OpenShift 4.3 on IBM Cloud. However you can also use CodeReady Containers to run OpenShift locally. To use OpenShift on IBM Cloud and LogDNA in lab 8, an IBM Cloud account is needed. It's free, doesn't expire and for the lite account no credit card is required. We will use preconfigured OpenShift on IBM Cloud clusters in this hands-on workshop.","title":"Lab 1: Create your Cloud Environment"},{"location":"pre-work/CLOUD_ACCOUNT/","text":"Access the Cluster \u00b6 In this section, you will login to your own IBM Cloud account, and then get access to a IBM Cloud Lab account which contains pre-provisioned clusters. Each lab attendee will be granted access to one cluster. Step 1: Setting up your IBM Cloud ID \u00b6 Log into IBM Cloud with an existing account: https://cloud.ibm.com OR Create your own: http://ibm.biz/nheidloff Step 2: Accessing the Cluster \u00b6 Instructors will provide a URL to a web app. Enter your IBMid (the email you used to sign up) and the lab key (also provided by the instructor). Follow the instructions on the next page. You will be added to the IBM Workshop account and granted access to a cluster. Note the name of your cluster. In the example below, it's TorontoMulticlientWorkshop31 . Back in IBM Cloud, refresh the IBM Cloud Dashboard . If required, switch to the 1840867-IBM account by clicking on the account selection drop down in the top nav bar. Click on Clusters in the Resource Summary tile. Under Clusters , click on the cluster that has been assigned to you. Launch the OpenShift web console and have a look around! You can come back to this dashboard throughout your lab.","title":"Access the Cluster"},{"location":"pre-work/CLOUD_ACCOUNT/#access-the-cluster","text":"In this section, you will login to your own IBM Cloud account, and then get access to a IBM Cloud Lab account which contains pre-provisioned clusters. Each lab attendee will be granted access to one cluster.","title":"Access the Cluster"},{"location":"pre-work/CLOUD_ACCOUNT/#step-1-setting-up-your-ibm-cloud-id","text":"Log into IBM Cloud with an existing account: https://cloud.ibm.com OR Create your own: http://ibm.biz/nheidloff","title":"Step 1: Setting up your IBM Cloud ID"},{"location":"pre-work/CLOUD_ACCOUNT/#step-2-accessing-the-cluster","text":"Instructors will provide a URL to a web app. Enter your IBMid (the email you used to sign up) and the lab key (also provided by the instructor). Follow the instructions on the next page. You will be added to the IBM Workshop account and granted access to a cluster. Note the name of your cluster. In the example below, it's TorontoMulticlientWorkshop31 . Back in IBM Cloud, refresh the IBM Cloud Dashboard . If required, switch to the 1840867-IBM account by clicking on the account selection drop down in the top nav bar. Click on Clusters in the Resource Summary tile. Under Clusters , click on the cluster that has been assigned to you. Launch the OpenShift web console and have a look around! You can come back to this dashboard throughout your lab.","title":"Step 2: Accessing the Cluster"},{"location":"pre-work/CLOUD_SHELL/","text":"IBM Cloud Shell \u00b6 For this workshop we'll be using the IBM Cloud Shell. The IBM Cloud Shell is a cloud-based shell workspace that you can access through your browser. It's preconfigured with the full IBM Cloud CLI and other 3 rd party CLIs like OpenShift's oc , Helm's helm and Kubernetes' kubectl . Step 1: Set up Terminal \u00b6 When using OpenShift on IBM Cloud no client side setup is required for this workshop. Instead we will use the IBM Cloud Shell (Beta) which comes with all necessary CLIs (command line tools). In your browser, login to the IBM Cloud Dashboard. Make sure you select your own account in the account list at the top, then click on the IBM Cloud Shell icon. Note: Your workspace includes 500 MB of temporary storage. This session will close after an hour of inactivity. If you don't have any active sessions for an hour or you reach the 30-hour weekly usage limit, your workspace data is removed. This is what you should see: When using OpenShift locally, you need a local terminal and the following tools: git curl oc mvn Java 9 or higher Step 2: Get the Code \u00b6 In the IBM Cloud Shell execute the following command: git clone https://github.com/IBM/cloud-native-starter.git Step 3. Get Access to OpenShift \u00b6 Open the IBM Cloud Dashboard . In the row at the top switch from your own account to the IBM account given to you by the instructor from the pulldown in the uper right corner. The select 'OpenShift' in the burger menu in the upper left corner followed by 'Clusters'. Click on your cluster. Open the OpenShift web console. From the dropdown menu in the upper right of the page, click 'Copy Login Command'. Click on 'Display Token', then copy and paste the command 'Log in with this token' into your terminal in the IBM Cloud Shell. Login to OpenShift in IBM Cloud Shell oc login https://c1XX-XX-X.containers.cloud.ibm.com:XXXXX --token=xxxxxx'","title":"Access IBM Cloud Shell"},{"location":"pre-work/CLOUD_SHELL/#ibm-cloud-shell","text":"For this workshop we'll be using the IBM Cloud Shell. The IBM Cloud Shell is a cloud-based shell workspace that you can access through your browser. It's preconfigured with the full IBM Cloud CLI and other 3 rd party CLIs like OpenShift's oc , Helm's helm and Kubernetes' kubectl .","title":"IBM Cloud Shell"},{"location":"pre-work/CLOUD_SHELL/#step-1-set-up-terminal","text":"When using OpenShift on IBM Cloud no client side setup is required for this workshop. Instead we will use the IBM Cloud Shell (Beta) which comes with all necessary CLIs (command line tools). In your browser, login to the IBM Cloud Dashboard. Make sure you select your own account in the account list at the top, then click on the IBM Cloud Shell icon. Note: Your workspace includes 500 MB of temporary storage. This session will close after an hour of inactivity. If you don't have any active sessions for an hour or you reach the 30-hour weekly usage limit, your workspace data is removed. This is what you should see: When using OpenShift locally, you need a local terminal and the following tools: git curl oc mvn Java 9 or higher","title":"Step 1: Set up Terminal"},{"location":"pre-work/CLOUD_SHELL/#step-2-get-the-code","text":"In the IBM Cloud Shell execute the following command: git clone https://github.com/IBM/cloud-native-starter.git","title":"Step 2: Get the Code"},{"location":"pre-work/CLOUD_SHELL/#step-3-get-access-to-openshift","text":"Open the IBM Cloud Dashboard . In the row at the top switch from your own account to the IBM account given to you by the instructor from the pulldown in the uper right corner. The select 'OpenShift' in the burger menu in the upper left corner followed by 'Clusters'. Click on your cluster. Open the OpenShift web console. From the dropdown menu in the upper right of the page, click 'Copy Login Command'. Click on 'Display Token', then copy and paste the command 'Log in with this token' into your terminal in the IBM Cloud Shell. Login to OpenShift in IBM Cloud Shell oc login https://c1XX-XX-X.containers.cloud.ibm.com:XXXXX --token=xxxxxx'","title":"Step 3. Get Access to OpenShift"}]}